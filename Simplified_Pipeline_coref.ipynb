{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 1: Split contract text into individual articles\n",
    "\n",
    "Using the regex method or elliott's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pipeline\n",
    "import plutil\n",
    "\n",
    "from smart_open import s3\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Old: Load from disk\n",
    "#canadian_pt_path = \"/home/research/corpora/contracts/canadian/txt\"\n",
    "#len(os.listdir(canadian_pt_path)) # 44,589 contracts *total*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "bucket_name = 'cuecon-textlab'\n",
    "bucket_prefix = 'home/research/corpora/contracts/canadian/txt/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#s3_contents = plutil.get_s3_contents(tl_bucket, bucket_prefix)\n",
    "#len(s3_contents)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#for key, content in s3.iter_bucket(bucket, prefix=prefix, accept_key=lambda key: '/201' in key, workers=1, key_limit=3):\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full output path: ..\\..\\..\\output\\analysis\\2022-08-jeff-python\n",
      "Loading filenames from cuecon-textlab/home/research/corpora/contracts/canadian/txt/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44590it [00:11, 3791.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 44589; files after language filter (['eng']): 35931\n",
      "35931 filenames loaded (0000102a_eng.txt ... 1498101a_eng.txt)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#pl = pipeline.Pipeline(\"canadian\", canadian_pt_path, lang_list=[\"eng\"],\n",
    "#                       sample_N=100, splitter=\"elliott\")\n",
    "s3_options = {\n",
    "    'bucket_name': 'cuecon-textlab',\n",
    "    'bucket_prefix': 'home/research/corpora/contracts/canadian/txt/'\n",
    "}\n",
    "pl = pipeline.Pipeline(\"canadian\", mode=\"s3\", mode_options=s3_options,\n",
    "                       output_dirname=\"2022-08-jeff-python\", lang_list=[\"eng\"],\n",
    "                       splitter=\"elliott\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "8658"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pl.excluded_fnames)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "('0945102c_fra.txt', '0923103c_fra.txt')"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.excluded_fnames[0], pl.excluded_fnames[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# done!\n",
    "#pl.split_contracts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#import json\n",
    "#fpath = \"../canadian_output/01_artsplit_elliott_json/0000102a.json\"\n",
    "#with open(fpath, 'r') as f:\n",
    "#    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\..\\output\\analysis\\2022-08-jeff-python\\01_artsplit_elliott_pkl\n"
     ]
    }
   ],
   "source": [
    "# Get the paths to the folders where the pkl and json files should be saved\n",
    "pkl_path = os.path.join(pl.get_output_path(), f\"01_artsplit_{pl.splitter}_pkl\")\n",
    "print(pkl_path)\n",
    "json_path = os.path.join(pl.get_output_path(), f\"01_artsplit_{pl.splitter}_json\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import detect_sections_elliott as dse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def split_contract(contract_id, contract_lang, contract_text):\n",
    "    arts, headers = dse.detect_sections(contract_text)\n",
    "    # Convert to the dict format for compatibility with regex splitter\n",
    "    # It looks like len(headers) is almost always greater than (often\n",
    "    # like double or triple) len(arts). So for now I'm ignoring headers\n",
    "    art_list = []\n",
    "    for i in range(len(arts)):\n",
    "        cur_art = arts[i]\n",
    "        cur_art_data = {'header': None, 'text': cur_art, 'section_num': i,\n",
    "                        'contract_id': contract_id, 'lang': contract_lang}\n",
    "        art_list.append(cur_art_data)\n",
    "    return art_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "save_json = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 100/35931 [00:06<40:47, 14.64it/s] \n"
     ]
    }
   ],
   "source": [
    "accept_rule = lambda fname: fname.endswith('_eng.txt')\n",
    "# Include key_limit=16 for debugging\n",
    "bucket_iter = s3.iter_bucket(bucket_name, prefix=bucket_prefix,\n",
    "                             accept_key=accept_rule, workers=16, key_limit=100)\n",
    "for fpath, content in tqdm(bucket_iter, total=pl.get_num_docs()):\n",
    "    fname = os.path.basename(fpath)\n",
    "    # First we get the info from the filename\n",
    "    fname_data = plutil.parse_fname(fname)\n",
    "    contract_prefix = fname_data['prefix']\n",
    "    contract_id = fname_data['id']\n",
    "    contract_lang = fname_data['lang']\n",
    "    # Now we process the content\n",
    "    #print(fname, len(content))\n",
    "    contract_text = content.decode('utf-8')\n",
    "    art_list = split_contract(contract_id, contract_lang, contract_text)\n",
    "    # And save the article list as .pkl (for internal use) and .json\n",
    "    # (for human reading)\n",
    "    pkl_fpath = os.path.join(pkl_path, f\"{contract_prefix}.pkl\")\n",
    "    plutil.safe_to_pickle(art_list, pkl_fpath)\n",
    "    #print(f\"Saved to {pkl_fpath}\")\n",
    "    if save_json:\n",
    "        json_fpath = os.path.join(json_path, f\"{contract_prefix}.json\")\n",
    "        plutil.safe_to_json(art_list, json_fpath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 2: Parse the articles using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting multiprocessing_logging\n",
      "  Downloading multiprocessing_logging-0.3.3-py2.py3-none-any.whl (8.3 kB)\n",
      "Installing collected packages: multiprocessing_logging\n",
      "Successfully installed multiprocessing_logging-0.3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install multiprocessing_logging"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neuralcoref\n",
      "  Downloading neuralcoref-4.0.tar.gz (368 kB)\n",
      "     -------------------------------------- 368.7/368.7 kB 1.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\jpjac\\lib\\site-packages (from neuralcoref) (1.22.4)\n",
      "Requirement already satisfied: boto3 in c:\\users\\jpjac\\lib\\site-packages (from neuralcoref) (1.24.59)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\jpjac\\lib\\site-packages (from neuralcoref) (2.27.1)\n",
      "Requirement already satisfied: spacy>=2.1.0 in c:\\users\\jpjac\\lib\\site-packages (from neuralcoref) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jpjac\\lib\\site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\jpjac\\lib\\site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jpjac\\lib\\site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jpjac\\lib\\site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.26.9)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\jpjac\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (0.7.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jpjac\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (21.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\jpjac\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (2.4.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\jpjac\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (3.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\jpjac\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (1.0.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\jpjac\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (0.6.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\jpjac\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (1.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\jpjac\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (1.0.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jpjac\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (3.1.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\jpjac\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (3.3.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\jpjac\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\jpjac\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (3.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\jpjac\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (4.64.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\jpjac\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (2.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\jpjac\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (8.0.16)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\jpjac\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (0.9.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\jpjac\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (2.0.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jpjac\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (60.2.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\jpjac\\lib\\site-packages (from boto3->neuralcoref) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\users\\jpjac\\lib\\site-packages (from boto3->neuralcoref) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.59 in c:\\users\\jpjac\\lib\\site-packages (from boto3->neuralcoref) (1.27.59)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\jpjac\\lib\\site-packages (from botocore<1.28.0,>=1.27.59->boto3->neuralcoref) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\jpjac\\lib\\site-packages (from packaging>=20.0->spacy>=2.1.0->neuralcoref) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\jpjac\\lib\\site-packages (from pathy>=0.3.5->spacy>=2.1.0->neuralcoref) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jpjac\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy>=2.1.0->neuralcoref) (4.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jpjac\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy>=2.1.0->neuralcoref) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\jpjac\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy>=2.1.0->neuralcoref) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jpjac\\lib\\site-packages (from jinja2->spacy>=2.1.0->neuralcoref) (2.1.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jpjac\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.59->boto3->neuralcoref) (1.16.0)\n",
      "Building wheels for collected packages: neuralcoref\n",
      "  Building wheel for neuralcoref (setup.py): started\n",
      "  Building wheel for neuralcoref (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for neuralcoref\n",
      "Failed to build neuralcoref\n",
      "Installing collected packages: neuralcoref\n",
      "  Running setup.py install for neuralcoref: started\n",
      "  Running setup.py install for neuralcoref: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [113 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.10\n",
      "  creating build\\lib.win-amd64-3.10\\neuralcoref\n",
      "  copying neuralcoref\\file_utils.py -> build\\lib.win-amd64-3.10\\neuralcoref\n",
      "  copying neuralcoref\\__init__.py -> build\\lib.win-amd64-3.10\\neuralcoref\n",
      "  creating build\\lib.win-amd64-3.10\\neuralcoref\\tests\n",
      "  copying neuralcoref\\tests\\test_neuralcoref.py -> build\\lib.win-amd64-3.10\\neuralcoref\\tests\n",
      "  copying neuralcoref\\tests\\__init__.py -> build\\lib.win-amd64-3.10\\neuralcoref\\tests\n",
      "  creating build\\lib.win-amd64-3.10\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\algorithm.py -> build\\lib.win-amd64-3.10\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\compat.py -> build\\lib.win-amd64-3.10\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\conllparser.py -> build\\lib.win-amd64-3.10\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\dataset.py -> build\\lib.win-amd64-3.10\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\document.py -> build\\lib.win-amd64-3.10\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\evaluator.py -> build\\lib.win-amd64-3.10\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\learn.py -> build\\lib.win-amd64-3.10\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\model.py -> build\\lib.win-amd64-3.10\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\utils.py -> build\\lib.win-amd64-3.10\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\__init__.py -> build\\lib.win-amd64-3.10\\neuralcoref\\train\n",
      "  running build_ext\n",
      "  creating build\\temp.win-amd64-3.10\n",
      "  creating build\\temp.win-amd64-3.10\\Release\n",
      "  creating build\\temp.win-amd64-3.10\\Release\\neuralcoref\n",
      "  \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.32.31326\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\\Python310\\include -IC:\\Users\\jpjac\\AppData\\Local\\Temp\\pip-install-9bgxlfxv\\neuralcoref_ce161625fc854c48948722682cdd27ba\\include -IC:\\Users\\jpjac\\include -IC:\\Python310\\include -IC:\\Python310\\Include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.32.31326\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\cppwinrt\" /EHsc /Tpneuralcoref/neuralcoref.cpp /Fobuild\\temp.win-amd64-3.10\\Release\\neuralcoref/neuralcoref.obj /Ox /EHsc\n",
      "  neuralcoref.cpp\n",
      "  C:\\Users\\jpjac\\AppData\\Local\\Temp\\pip-install-9bgxlfxv\\neuralcoref_ce161625fc854c48948722682cdd27ba\\include\\numpy\\npy_deprecated_api.h(8) : Warning Msg: Using deprecated NumPy API, disable it by #defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\n",
      "  neuralcoref/neuralcoref.cpp(6050): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(6904): warning C4244: '=': conversion from 'double' to 'float', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(8989): warning C4018: '>': signed/unsigned mismatch\n",
      "  neuralcoref/neuralcoref.cpp(9046): warning C4018: '<': signed/unsigned mismatch\n",
      "  neuralcoref/neuralcoref.cpp(10177): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(10252): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(17150): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(18283): warning C4244: 'argument': conversion from 'uint64_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(18283): warning C4244: 'argument': conversion from 'uint64_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(18526): warning C4244: '=': conversion from 'int' to 'float', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(18538): warning C4244: '=': conversion from 'int' to 'float', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(18550): warning C4244: '=': conversion from 'int' to 'float', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(18586): warning C4244: '=': conversion from 'double' to 'float', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(18595): warning C4244: 'argument': conversion from 'uint64_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(18622): warning C4244: '=': conversion from 'double' to 'float', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(18634): warning C4244: '=': conversion from 'int' to 'float', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(18937): warning C4244: '=': conversion from 'double' to 'float', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(30715): warning C4244: 'argument': conversion from '__pyx_t_5spacy_8typedefs_attr_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(32160): warning C4244: '=': conversion from '__pyx_t_5spacy_8typedefs_attr_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(32179): warning C4244: '=': conversion from '__pyx_t_5spacy_8typedefs_attr_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(32198): warning C4244: '=': conversion from '__pyx_t_5spacy_8typedefs_attr_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(32234): warning C4244: '=': conversion from '__pyx_t_5spacy_8typedefs_attr_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(47321): error C2105: '++' needs l-value\n",
      "  neuralcoref/neuralcoref.cpp(47323): error C2105: '--' needs l-value\n",
      "  neuralcoref/neuralcoref.cpp(47610): error C2105: '++' needs l-value\n",
      "  neuralcoref/neuralcoref.cpp(47612): error C2105: '--' needs l-value\n",
      "  neuralcoref/neuralcoref.cpp(47849): error C2105: '++' needs l-value\n",
      "  neuralcoref/neuralcoref.cpp(47851): error C2105: '--' needs l-value\n",
      "  neuralcoref/neuralcoref.cpp(49313): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49340): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49346): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49352): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49358): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49364): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49370): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49376): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49382): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49388): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49394): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49400): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49406): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49412): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49418): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49424): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49430): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49436): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49442): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49448): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49456): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49461): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49476): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49489): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(50396): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(51983): warning C4996: '_PyUnicode_get_wstr_length': deprecated in 3.3\n",
      "  neuralcoref/neuralcoref.cpp(51999): warning C4996: '_PyUnicode_get_wstr_length': deprecated in 3.3\n",
      "  neuralcoref/neuralcoref.cpp(53921): error C3861: '_PyGen_Send': identifier not found\n",
      "  neuralcoref/neuralcoref.cpp(53926): error C3861: '_PyGen_Send': identifier not found\n",
      "  neuralcoref/neuralcoref.cpp(54010): error C3861: '_PyGen_Send': identifier not found\n",
      "  neuralcoref/neuralcoref.cpp(55241): warning C4996: 'PyUnicode_FromUnicode': deprecated in 3.3\n",
      "  error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2022\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.32.31326\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for neuralcoref\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Running setup.py install for neuralcoref did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [115 lines of output]\n",
      "  running install\n",
      "  C:\\Users\\jpjac\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "    warnings.warn(\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.10\n",
      "  creating build\\lib.win-amd64-3.10\\neuralcoref\n",
      "  copying neuralcoref\\file_utils.py -> build\\lib.win-amd64-3.10\\neuralcoref\n",
      "  copying neuralcoref\\__init__.py -> build\\lib.win-amd64-3.10\\neuralcoref\n",
      "  creating build\\lib.win-amd64-3.10\\neuralcoref\\tests\n",
      "  copying neuralcoref\\tests\\test_neuralcoref.py -> build\\lib.win-amd64-3.10\\neuralcoref\\tests\n",
      "  copying neuralcoref\\tests\\__init__.py -> build\\lib.win-amd64-3.10\\neuralcoref\\tests\n",
      "  creating build\\lib.win-amd64-3.10\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\algorithm.py -> build\\lib.win-amd64-3.10\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\compat.py -> build\\lib.win-amd64-3.10\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\conllparser.py -> build\\lib.win-amd64-3.10\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\dataset.py -> build\\lib.win-amd64-3.10\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\document.py -> build\\lib.win-amd64-3.10\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\evaluator.py -> build\\lib.win-amd64-3.10\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\learn.py -> build\\lib.win-amd64-3.10\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\model.py -> build\\lib.win-amd64-3.10\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\utils.py -> build\\lib.win-amd64-3.10\\neuralcoref\\train\n",
      "  copying neuralcoref\\train\\__init__.py -> build\\lib.win-amd64-3.10\\neuralcoref\\train\n",
      "  running build_ext\n",
      "  creating build\\temp.win-amd64-3.10\n",
      "  creating build\\temp.win-amd64-3.10\\Release\n",
      "  creating build\\temp.win-amd64-3.10\\Release\\neuralcoref\n",
      "  \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.32.31326\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\\Python310\\include -IC:\\Users\\jpjac\\AppData\\Local\\Temp\\pip-install-9bgxlfxv\\neuralcoref_ce161625fc854c48948722682cdd27ba\\include -IC:\\Users\\jpjac\\include -IC:\\Python310\\include -IC:\\Python310\\Include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.32.31326\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.19041.0\\\\cppwinrt\" /EHsc /Tpneuralcoref/neuralcoref.cpp /Fobuild\\temp.win-amd64-3.10\\Release\\neuralcoref/neuralcoref.obj /Ox /EHsc\n",
      "  neuralcoref.cpp\n",
      "  C:\\Users\\jpjac\\AppData\\Local\\Temp\\pip-install-9bgxlfxv\\neuralcoref_ce161625fc854c48948722682cdd27ba\\include\\numpy\\npy_deprecated_api.h(8) : Warning Msg: Using deprecated NumPy API, disable it by #defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\n",
      "  neuralcoref/neuralcoref.cpp(6050): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(6904): warning C4244: '=': conversion from 'double' to 'float', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(8989): warning C4018: '>': signed/unsigned mismatch\n",
      "  neuralcoref/neuralcoref.cpp(9046): warning C4018: '<': signed/unsigned mismatch\n",
      "  neuralcoref/neuralcoref.cpp(10177): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(10252): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(17150): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(18283): warning C4244: 'argument': conversion from 'uint64_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(18283): warning C4244: 'argument': conversion from 'uint64_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(18526): warning C4244: '=': conversion from 'int' to 'float', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(18538): warning C4244: '=': conversion from 'int' to 'float', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(18550): warning C4244: '=': conversion from 'int' to 'float', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(18586): warning C4244: '=': conversion from 'double' to 'float', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(18595): warning C4244: 'argument': conversion from 'uint64_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(18622): warning C4244: '=': conversion from 'double' to 'float', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(18634): warning C4244: '=': conversion from 'int' to 'float', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(18937): warning C4244: '=': conversion from 'double' to 'float', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(30715): warning C4244: 'argument': conversion from '__pyx_t_5spacy_8typedefs_attr_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(32160): warning C4244: '=': conversion from '__pyx_t_5spacy_8typedefs_attr_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(32179): warning C4244: '=': conversion from '__pyx_t_5spacy_8typedefs_attr_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(32198): warning C4244: '=': conversion from '__pyx_t_5spacy_8typedefs_attr_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(32234): warning C4244: '=': conversion from '__pyx_t_5spacy_8typedefs_attr_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(47321): error C2105: '++' needs l-value\n",
      "  neuralcoref/neuralcoref.cpp(47323): error C2105: '--' needs l-value\n",
      "  neuralcoref/neuralcoref.cpp(47610): error C2105: '++' needs l-value\n",
      "  neuralcoref/neuralcoref.cpp(47612): error C2105: '--' needs l-value\n",
      "  neuralcoref/neuralcoref.cpp(47849): error C2105: '++' needs l-value\n",
      "  neuralcoref/neuralcoref.cpp(47851): error C2105: '--' needs l-value\n",
      "  neuralcoref/neuralcoref.cpp(49313): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49340): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49346): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49352): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49358): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49364): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49370): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49376): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49382): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49388): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49394): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49400): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49406): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49412): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49418): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49424): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49430): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49436): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49442): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49448): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49456): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49461): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49476): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(49489): error C2039: 'tp_print': is not a member of '_typeobject'\n",
      "  C:\\Python310\\include\\cpython/object.h(191): note: see declaration of '_typeobject'\n",
      "  neuralcoref/neuralcoref.cpp(50396): warning C4267: '=': conversion from 'size_t' to 'int', possible loss of data\n",
      "  neuralcoref/neuralcoref.cpp(51983): warning C4996: '_PyUnicode_get_wstr_length': deprecated in 3.3\n",
      "  neuralcoref/neuralcoref.cpp(51999): warning C4996: '_PyUnicode_get_wstr_length': deprecated in 3.3\n",
      "  neuralcoref/neuralcoref.cpp(53921): error C3861: '_PyGen_Send': identifier not found\n",
      "  neuralcoref/neuralcoref.cpp(53926): error C3861: '_PyGen_Send': identifier not found\n",
      "  neuralcoref/neuralcoref.cpp(54010): error C3861: '_PyGen_Send': identifier not found\n",
      "  neuralcoref/neuralcoref.cpp(55241): warning C4996: 'PyUnicode_FromUnicode': deprecated in 3.3\n",
      "  error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2022\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.32.31326\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "Encountered error while trying to install package.\n",
      "\n",
      "neuralcoref\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n"
     ]
    }
   ],
   "source": [
    "!pip install neuralcoref"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neuralcoref'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[1;32mIn [20]\u001B[0m, in \u001B[0;36m<cell line: 13>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# Local imports\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpipeline\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmain02_spacy_parse\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Dropbox\\Labor_Contracts_Canada\\analysis\\code\\code-python\\simplified_pipeline\\main02_spacy_parse.py:12\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmultiprocessing_logging\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mspacy\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mneuralcoref\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# Local imports\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpipeline\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'neuralcoref'"
     ]
    }
   ],
   "source": [
    "# Python imports\n",
    "import functools\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# 3rd party imports\n",
    "import joblib\n",
    "\n",
    "# Local imports\n",
    "import pipeline\n",
    "import main02_spacy_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# And set it to work with spacy's use of multiprocessing\n",
    "import multiprocessing_logging\n",
    "multiprocessing_logging.install_mp_handler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load the actual spaCy NLP object (`nlp_eng`), and extend it to include neuralcoref annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Loading spaCy core model\")\n",
    "nlp_eng = spacy.load('en_core_web_md', disable=[\"ner\"])\n",
    "print(\"Loading spaCy coref model. May take a while...\")\n",
    "neuralcoref.add_to_pipe(nlp_eng);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Annoying but necessary additional step: adding \"contract_id\" and \"art_num\" attributes to spacy's Doc class, so that we can serialize and deserialize without headaches [x__x]\n",
    "\n",
    "See https://spacy.io/usage/processing-pipelines#custom-components-attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The force=True is just so that we can change (e.g.) the names or default values and overwrite the extensions\n",
    "# (otherwise this would always cause an Exception)\n",
    "spacy.tokens.Doc.set_extension(\"contract_id\", default=None, force=True)\n",
    "spacy.tokens.Doc.set_extension(\"article_num\", default=None, force=True)\n",
    "spacy.tokens.Doc.set_extension(\"coref_list\", default=[], force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Aaand yet another necessary workaround...\n",
    "\n",
    "See https://github.com/huggingface/neuralcoref/issues/82#issuecomment-569431503\n",
    "\n",
    "[update: holding off on this one actually, since I need the coref data... ugh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cr_test_doc = nlp_eng(u'My sister has a dog. She loves him.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mentions = [\n",
    "    {\n",
    "        \"start\": mention.start_char,\n",
    "        \"end\": mention.end_char,\n",
    "        \"text\": mention.text,\n",
    "        \"resolved\": cluster.main.text,\n",
    "    }\n",
    "    for cluster in cr_test_doc._.coref_clusters\n",
    "    for mention in cluster.mentions\n",
    "]\n",
    "#clusters = list(\n",
    "#    list(span.text for span in cluster)\n",
    "#    for cluster in cr_test_doc._.coref_clusters\n",
    "#)\n",
    "#resolved = cr_test_doc._.coref_resolved\n",
    "#response = {}\n",
    "#response[\"mentions\"] = mentions\n",
    "#response[\"clusters\"] = clusters\n",
    "#response[\"resolved\"] = resolved\n",
    "mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "K I guess we'll use this representation to avoid the serialization errors :|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# All the neuralcoref attributes for a doc, for future reference:\n",
    "#cr_test_doc._.has_coref\n",
    "#cr_test_doc._.coref_resolved\n",
    "#cr_test_doc._.coref_clusters\n",
    "#cr_test_doc._.coref_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#for cluster in cr_test_doc._.coref_clusters:\n",
    "#    print(f\"===== #{cluster.i}\")\n",
    "#    print(cluster)\n",
    "#    print(f\"main: '{cluster.main}'\")\n",
    "#    print(cluster.mentions)\n",
    "#    for mention in cluster.mentions:\n",
    "#        print(mention)\n",
    "#        print(mention.start)\n",
    "#        print(mention.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def stream_art_data(test_N=None):\n",
    "    \"\"\"\n",
    "    test_N: If set to an int, the function will only yield article data for the first `test_N` contracts.\n",
    "            Otherwise, if set to None, article data for all contracts is yielded.\n",
    "    \"\"\"\n",
    "    art_data_fpaths = glob.glob(\"../canadian_output/01_artsplit_elliott_json/*.json\")\n",
    "    # Loop over contracts\n",
    "    for fnum, fpath in enumerate(art_data_fpaths):\n",
    "        if test_N is not None and fnum >= test_N:\n",
    "            # We've already yielded the first `test_N` contracts, so terminate\n",
    "            break\n",
    "        with open(fpath, 'r') as f:\n",
    "            all_articles = json.load(f)\n",
    "        # Now loop over the articles\n",
    "        for cur_article in all_articles:\n",
    "            # We want to yield tuples of (string, {contract_id, article_num})\n",
    "            art_str = cur_article['text']\n",
    "            art_data = {'contract_id':cur_article['contract_id'],\n",
    "                        'article_num':cur_article['section_num']}\n",
    "            yield (art_str, art_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#art_data_fpaths = glob.glob(\"../canadian_output/01_artsplit_elliott_json/*.json\")\n",
    "#first_fpath = art_data_fpaths[0]\n",
    "#with open(first_fpath, 'r') as f:\n",
    "#    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_unserializable_results(doc):\n",
    "    doc.user_data = {}\n",
    "    for x in dir(doc._):\n",
    "        if x in ['get', 'set', 'has']: continue\n",
    "        setattr(doc._, x, None)\n",
    "    for token in doc:\n",
    "        for x in dir(token._):\n",
    "            if x in ['get', 'set', 'has']: continue\n",
    "            setattr(token._, x, None)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_coref_data(doc_obj):\n",
    "    mentions = [\n",
    "        {\n",
    "            \"start\": mention.start_char,\n",
    "            \"end\": mention.end_char,\n",
    "            \"text\": mention.text,\n",
    "            \"resolved\": cluster.main.text,\n",
    "        }\n",
    "        for cluster in doc_obj._.coref_clusters\n",
    "        for mention in cluster.mentions\n",
    "    ]\n",
    "    return mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def transform_texts(nlp, batch_id, batch_tuples, output_dir):\n",
    "    # Using spacy's \"DocBin\" functionality: see https://spacy.io/usage/saving-loading#docs\n",
    "    batch_bin = spacy.tokens.DocBin(store_user_data=True)\n",
    "    #print(nlp.pipe_names)\n",
    "    output_fpath = os.path.join(output_dir, f\"{batch_id}.bin\")\n",
    "    if os.path.isfile(output_fpath):  # return None in case same batch is called again\n",
    "        return None\n",
    "    print(\"Processing batch\", batch_id)\n",
    "    for art_doc, art_meta in nlp.pipe(batch_tuples, as_tuples=True):\n",
    "        # This is the weird part where we now have to change contract_id and art_num\n",
    "        # from being metadata to being attributes of the spacy Doc objects themselves\n",
    "        contract_id = art_meta[\"contract_id\"]\n",
    "        article_num = art_meta[\"article_num\"]\n",
    "        art_doc._.contract_id = contract_id\n",
    "        art_doc._.article_num = article_num\n",
    "        # And now we don't need the meta object anymore, since it's encoded in the Doc itself\n",
    "        # But next we need to get a serializable representation of the detected corefs\n",
    "        art_doc._.coref_list = get_coref_data(art_doc)\n",
    "        # Ok now we can get rid of the original coref attributes that break the data\n",
    "        art_doc = remove_unserializable_results(art_doc)\n",
    "        batch_bin.add(art_doc)\n",
    "    # Now we can use spacy's serialization methods [joblib basically fails at serializing\n",
    "    # spacy Docs for various reasons]\n",
    "    # [see https://spacy.io/usage/saving-loading#docs]\n",
    "    batch_bytes = batch_bin.to_bytes()\n",
    "    # And save the bytes object to file\n",
    "    with open(output_fpath, \"wb\") as f:\n",
    "        f.write(batch_bytes)\n",
    "    print(\"Saved {} texts to {}.bin\".format(len(batch_tuples), batch_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Trying to use multiprocessing like in\n",
    "# https://spacy.io/usage/examples#multi-processing\n",
    "#output_dir = \"./mp_test\"\n",
    "output_dir = \"./mp_full\"\n",
    "#art_tuple_stream = stream_art_data(test_N=50)\n",
    "art_tuple_stream = stream_art_data()\n",
    "\n",
    "print(\"Processing texts...\")\n",
    "batch_size = 1000\n",
    "#batch_size = 200\n",
    "n_jobs = 16\n",
    "art_partitions = spacy.util.minibatch(art_tuple_stream, size=batch_size)\n",
    "executor = joblib.Parallel(n_jobs=n_jobs, backend=\"multiprocessing\", prefer=\"processes\")\n",
    "do = joblib.delayed(functools.partial(transform_texts, nlp_eng))\n",
    "tasks = (do(i, batch_tuples, output_dir) for i, batch_tuples in enumerate(art_partitions))\n",
    "executor(tasks);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Test that it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bin_fpath = \"./mp_test/6.bin\"\n",
    "with open(bin_fpath, \"rb\") as f:\n",
    "    loaded_bytes = f.read()\n",
    "loaded_bin = spacy.tokens.DocBin().from_bytes(loaded_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "doc_iter = loaded_bin.get_docs(nlp_eng.vocab)\n",
    "doc_list = list(doc_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "doc_list[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "processed_arts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#for art_nlp, art_meta in nlp_eng.pipe(stream_art_data(test_N=1), as_tuples=True):\n",
    "#    logger.info(f\"Finished processing: {art_meta}\")\n",
    "#    processed_arts.append((art_nlp, art_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(processed_arts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "type(processed_arts[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "statement_list = main02_parse_articles.parallel_parse(pl, nlp_eng, stream_art_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}