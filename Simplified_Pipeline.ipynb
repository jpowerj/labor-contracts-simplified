{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 1: Split contract text into individual articles\n",
    "\n",
    "Using the regex method or elliott's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44589"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Old: Load from disk\n",
    "canadian_pt_path = \"/home/research/corpora/contracts/canadian/txt\"\n",
    "len(os.listdir(canadian_pt_path)) # 44,589 contracts *total*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# New: Stream from S3\n",
    "import boto3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing buckets:\n",
      "  cuecon-textlab\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the list of existing buckets\n",
    "s3 = boto3.client('s3')\n",
    "response = s3.list_buckets()\n",
    "\n",
    "# Output the bucket names\n",
    "print('Existing buckets:')\n",
    "for bucket in response['Buckets']:\n",
    "    print(f'  {bucket[\"Name\"]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Pick out only English-language contracts\n",
    "#len(txt_fpaths) #35,931 *English-language* contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#pl = pipeline.Pipeline(\"canadian\", canadian_pt_path, lang_list=[\"eng\"],\n",
    "#                       sample_N=100, splitter=\"elliott\")\n",
    "pl = pipeline.Pipeline(\"canadian\", canadian_pt_path, lang_list=[\"eng\"],\n",
    "                       splitter=\"elliott\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# done!\n",
    "#pl.split_contracts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#import json\n",
    "#fpath = \"../canadian_output/01_artsplit_elliott_json/0000102a.json\"\n",
    "#with open(fpath, 'r') as f:\n",
    "#    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 2: Parse the articles using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import functools\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# 3rd party imports\n",
    "import joblib\n",
    "\n",
    "# Local imports\n",
    "import pipeline\n",
    "import main02_parse_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# And set it to work with spacy's use of multiprocessing\n",
    "import multiprocessing_logging\n",
    "multiprocessing_logging.install_mp_handler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "canadian_pt_path = \"/home/research/corpora/contracts/canadian/txt\"\n",
    "len(os.listdir(canadian_pt_path)) # 44,589 contracts *total*\n",
    "pl = pipeline.Pipeline(\"canadian\", canadian_pt_path, lang_list=[\"eng\"],\n",
    "                       splitter=\"elliott\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load the actual spaCy NLP object (`nlp_eng`), and extend it to include neuralcoref annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy core model\n",
      "Loading spaCy coref model. May take a while...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading spaCy core model\")\n",
    "nlp_eng = spacy.load('en_core_web_md', disable=[\"ner\"])\n",
    "print(\"Loading spaCy coref model. May take a while...\")\n",
    "neuralcoref.add_to_pipe(nlp_eng);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Annoying but necessary additional step: adding \"contract_id\" and \"art_num\" attributes to spacy's Doc class, so that we can serialize and deserialize without headaches [x__x]\n",
    "\n",
    "See https://spacy.io/usage/processing-pipelines#custom-components-attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The force=True is just so that we can change (e.g.) the names or default values and overwrite the extensions\n",
    "# (otherwise this would always cause an Exception)\n",
    "spacy.tokens.Doc.set_extension(\"contract_id\", default=None, force=True)\n",
    "spacy.tokens.Doc.set_extension(\"article_num\", default=None, force=True)\n",
    "spacy.tokens.Doc.set_extension(\"coref_list\", default=[], force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Aaand yet another necessary workaround...\n",
    "\n",
    "See https://github.com/huggingface/neuralcoref/issues/82#issuecomment-569431503\n",
    "\n",
    "[update: holding off on this one actually, since I need the coref data... ugh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cr_test_doc = nlp_eng(u'My sister has a dog. She loves him.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0, 'end': 9, 'text': 'My sister', 'resolved': 'My sister'},\n",
       " {'start': 21, 'end': 24, 'text': 'She', 'resolved': 'My sister'},\n",
       " {'start': 14, 'end': 19, 'text': 'a dog', 'resolved': 'a dog'},\n",
       " {'start': 31, 'end': 34, 'text': 'him', 'resolved': 'a dog'}]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions = [\n",
    "    {\n",
    "        \"start\": mention.start_char,\n",
    "        \"end\": mention.end_char,\n",
    "        \"text\": mention.text,\n",
    "        \"resolved\": cluster.main.text,\n",
    "    }\n",
    "    for cluster in cr_test_doc._.coref_clusters\n",
    "    for mention in cluster.mentions\n",
    "]\n",
    "#clusters = list(\n",
    "#    list(span.text for span in cluster)\n",
    "#    for cluster in cr_test_doc._.coref_clusters\n",
    "#)\n",
    "#resolved = cr_test_doc._.coref_resolved\n",
    "#response = {}\n",
    "#response[\"mentions\"] = mentions\n",
    "#response[\"clusters\"] = clusters\n",
    "#response[\"resolved\"] = resolved\n",
    "mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "K I guess we'll use this representation to avoid the serialization errors :|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# All the neuralcoref attributes for a doc, for future reference:\n",
    "#cr_test_doc._.has_coref\n",
    "#cr_test_doc._.coref_resolved\n",
    "#cr_test_doc._.coref_clusters\n",
    "#cr_test_doc._.coref_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#for cluster in cr_test_doc._.coref_clusters:\n",
    "#    print(f\"===== #{cluster.i}\")\n",
    "#    print(cluster)\n",
    "#    print(f\"main: '{cluster.main}'\")\n",
    "#    print(cluster.mentions)\n",
    "#    for mention in cluster.mentions:\n",
    "#        print(mention)\n",
    "#        print(mention.start)\n",
    "#        print(mention.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def stream_art_data(test_N=None):\n",
    "    \"\"\"\n",
    "    test_N: If set to an int, the function will only yield article data for the first `test_N` contracts.\n",
    "            Otherwise, if set to None, article data for all contracts is yielded.\n",
    "    \"\"\"\n",
    "    art_data_fpaths = glob.glob(\"../canadian_output/01_artsplit_elliott_json/*.json\")\n",
    "    # Loop over contracts\n",
    "    for fnum, fpath in enumerate(art_data_fpaths):\n",
    "        if test_N is not None and fnum >= test_N:\n",
    "            # We've already yielded the first `test_N` contracts, so terminate\n",
    "            break\n",
    "        with open(fpath, 'r') as f:\n",
    "            all_articles = json.load(f)\n",
    "        # Now loop over the articles\n",
    "        for cur_article in all_articles:\n",
    "            # We want to yield tuples of (string, {contract_id, article_num})\n",
    "            art_str = cur_article['text']\n",
    "            art_data = {'contract_id':cur_article['contract_id'],\n",
    "                        'article_num':cur_article['section_num']}\n",
    "            yield (art_str, art_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#art_data_fpaths = glob.glob(\"../canadian_output/01_artsplit_elliott_json/*.json\")\n",
    "#first_fpath = art_data_fpaths[0]\n",
    "#with open(first_fpath, 'r') as f:\n",
    "#    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_unserializable_results(doc):\n",
    "    doc.user_data = {}\n",
    "    for x in dir(doc._):\n",
    "        if x in ['get', 'set', 'has']: continue\n",
    "        setattr(doc._, x, None)\n",
    "    for token in doc:\n",
    "        for x in dir(token._):\n",
    "            if x in ['get', 'set', 'has']: continue\n",
    "            setattr(token._, x, None)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_coref_data(doc_obj):\n",
    "    mentions = [\n",
    "        {\n",
    "            \"start\": mention.start_char,\n",
    "            \"end\": mention.end_char,\n",
    "            \"text\": mention.text,\n",
    "            \"resolved\": cluster.main.text,\n",
    "        }\n",
    "        for cluster in doc_obj._.coref_clusters\n",
    "        for mention in cluster.mentions\n",
    "    ]\n",
    "    return mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def transform_texts(nlp, batch_id, batch_tuples, output_dir):\n",
    "    # Using spacy's \"DocBin\" functionality: see https://spacy.io/usage/saving-loading#docs\n",
    "    batch_bin = spacy.tokens.DocBin(store_user_data=True)\n",
    "    #print(nlp.pipe_names)\n",
    "    output_fpath = os.path.join(output_dir, f\"{batch_id}.bin\")\n",
    "    if os.path.isfile(output_fpath):  # return None in case same batch is called again\n",
    "        return None\n",
    "    print(\"Processing batch\", batch_id)\n",
    "    for art_doc, art_meta in nlp.pipe(batch_tuples, as_tuples=True):\n",
    "        # This is the weird part where we now have to change contract_id and art_num\n",
    "        # from being metadata to being attributes of the spacy Doc objects themselves\n",
    "        contract_id = art_meta[\"contract_id\"]\n",
    "        article_num = art_meta[\"article_num\"]\n",
    "        art_doc._.contract_id = contract_id\n",
    "        art_doc._.article_num = article_num\n",
    "        # And now we don't need the meta object anymore, since it's encoded in the Doc itself\n",
    "        # But next we need to get a serializable representation of the detected corefs\n",
    "        art_doc._.coref_list = get_coref_data(art_doc)\n",
    "        # Ok now we can get rid of the original coref attributes that break the data\n",
    "        art_doc = remove_unserializable_results(art_doc)\n",
    "        batch_bin.add(art_doc)\n",
    "    # Now we can use spacy's serialization methods [joblib basically fails at serializing\n",
    "    # spacy Docs for various reasons]\n",
    "    # [see https://spacy.io/usage/saving-loading#docs]\n",
    "    batch_bytes = batch_bin.to_bytes()\n",
    "    # And save the bytes object to file\n",
    "    with open(output_fpath, \"wb\") as f:\n",
    "        f.write(batch_bytes)\n",
    "    print(\"Saved {} texts to {}.bin\".format(len(batch_tuples), batch_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing texts...\n",
      "Processing batch 0\n",
      "Processing batch 1\n",
      "Processing batch 2\n",
      "Processing batch 3\n",
      "Saved 200 texts to 0.bin\n",
      "Processing batch 4\n",
      "Saved 200 texts to 3.bin\n",
      "Processing batch 5\n",
      "Saved 200 texts to 1.bin\n",
      "Processing batch 6\n",
      "Saved 200 texts to 4.bin\n",
      "Saved 200 texts to 2.bin\n",
      "Saved 200 texts to 5.bin\n",
      "Saved 77 texts to 6.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying to use multiprocessing like in\n",
    "# https://spacy.io/usage/examples#multi-processing\n",
    "#output_dir = \"./mp_test\"\n",
    "output_dir = \"./mp_full\"\n",
    "#art_tuple_stream = stream_art_data(test_N=50)\n",
    "art_tuple_stream = stream_art_data()\n",
    "\n",
    "print(\"Processing texts...\")\n",
    "batch_size = 1000\n",
    "#batch_size = 200\n",
    "n_jobs = 16\n",
    "art_partitions = spacy.util.minibatch(art_tuple_stream, size=batch_size)\n",
    "executor = joblib.Parallel(n_jobs=n_jobs, backend=\"multiprocessing\", prefer=\"processes\")\n",
    "do = joblib.delayed(functools.partial(transform_texts, nlp_eng))\n",
    "tasks = (do(i, batch_tuples, output_dir) for i, batch_tuples in enumerate(art_partitions))\n",
    "executor(tasks);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Test that it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bin_fpath = \"./mp_test/6.bin\"\n",
    "with open(bin_fpath, \"rb\") as f:\n",
    "    loaded_bytes = f.read()\n",
    "loaded_bin = spacy.tokens.DocBin().from_bytes(loaded_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "doc_iter = loaded_bin.get_docs(nlp_eng.vocab)\n",
    "doc_list = list(doc_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Arbitration. 16.Ol The parties have agreed to the concept of pursuing alternative means of dispute resolution. This may include grievance mediation, \"referee\" systems, or other similar systems. 16.02 a) When either party decides to submit a grievance to arbitration as per Article 1504 b), then the other party shall be so notified in writing by registered mail.b) The parties have agreed on a panel of arbitrators to be used, if required, during the term of this agreement. c) If the parties fail to agree on the selection of a singlearbitrator from among the panel, they shall request the honourable Minister of Labour of the Province of Manitoba to make the appointment from among the said panel.d) In the event that the arbitrators provided for in this section are not available to preside as arbitrator, the parties agree that they will request the Honourable Minister of Labour of the Province of Manitoba to appoint a temporary replacement.16.03 No person shall be appointed as an arbitrator who has been involved previously in an attempt to negotiate or settle the grievance. 16.04 In the case of arbitration, the conferring parties may have the assistance of the employee or the employees concerned and any necessary witnesses, and all necessary arrangements will be made to view disputed operations and to confer with the necessary witnesses. 16.05 The remuneration and expenses of the Arbitrator shall be borne equally by the Company and the Union. Witness fees and allowances shall be paid by the party calling the witness. 16.06 The proceedings and decision of the Arbitration Board shall be expedited by the Company and the Union. The decision of the majority of such Board shall be final and binding upon the parties hereto, but the arbitrator shall not be authorized to make, nor shall they make, any decision or recommendation inconsistent with the provisions of this Agreement, nor alter, add to, modify or amend any part of the Agreement, or make any general changes, such as changes in wage rates, nor deal with any matter not covered by this Agreement. 16.07 In all cases of interpretation or arbitration of the terms of this Agreement, the text of the original signed copies shall govern. "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "processed_arts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#for art_nlp, art_meta in nlp_eng.pipe(stream_art_data(test_N=1), as_tuples=True):\n",
    "#    logger.info(f\"Finished processing: {art_meta}\")\n",
    "#    processed_arts.append((art_nlp, art_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_arts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(processed_arts[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing articles in parallel via parallel_parse()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-ff4c854a6442>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mstatement_list\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmain02_parse_articles\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparallel_parse\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnlp_eng\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstream_art_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/home/research/projects/ashmacnaidu/pipeline_code_new/main02_parse_articles.py\u001B[0m in \u001B[0;36mparallel_parse\u001B[0;34m(pl, nlp_eng, data_stream_fn)\u001B[0m\n\u001B[1;32m    111\u001B[0m     \u001B[0;31m#print(next(pl.stream_art_data()))\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    112\u001B[0m     \u001B[0;31m#for article_nlp in nlp_eng.pipe(art_list, n_threads=8):\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 113\u001B[0;31m     \u001B[0;32mfor\u001B[0m \u001B[0mart_nlp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mart_meta\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mnlp_eng\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpipe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_stream_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mas_tuples\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    114\u001B[0m         \u001B[0mcontract_id\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mart_meta\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"contract_id\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    115\u001B[0m         \u001B[0;31m#print(f\"Processing contract_id {contract_id}\")\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/language.py\u001B[0m in \u001B[0;36mpipe\u001B[0;34m(self, texts, as_tuples, n_threads, batch_size, disable, cleanup, component_cfg)\u001B[0m\n\u001B[1;32m    681\u001B[0m                 \u001B[0mcomponent_cfg\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcomponent_cfg\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    682\u001B[0m             )\n\u001B[0;32m--> 683\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mdoc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mizip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdocs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontexts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    684\u001B[0m                 \u001B[0;32myield\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mdoc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    685\u001B[0m             \u001B[0;32mreturn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/language.py\u001B[0m in \u001B[0;36mpipe\u001B[0;34m(self, texts, as_tuples, n_threads, batch_size, disable, cleanup, component_cfg)\u001B[0m\n\u001B[1;32m    709\u001B[0m         \u001B[0moriginal_strings_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    710\u001B[0m         \u001B[0mnr_seen\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 711\u001B[0;31m         \u001B[0;32mfor\u001B[0m \u001B[0mdoc\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdocs\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    712\u001B[0m             \u001B[0;32myield\u001B[0m \u001B[0mdoc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    713\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mcleanup\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32mneuralcoref.pyx\u001B[0m in \u001B[0;36mpipe\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mneuralcoref.pyx\u001B[0m in \u001B[0;36mneuralcoref.neuralcoref.NeuralCoref.predict\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/model.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    167\u001B[0m             \u001B[0mMust\u001B[0m \u001B[0mmatch\u001B[0m \u001B[0mexpected\u001B[0m \u001B[0mshape\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    168\u001B[0m         \"\"\"\n\u001B[0;32m--> 169\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    170\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    171\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mpipe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstream\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m128\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\u001B[0m in \u001B[0;36mpredict\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m     38\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mlayer\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_layers\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 40\u001B[0;31m             \u001B[0mX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlayer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     41\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/model.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    167\u001B[0m             \u001B[0mMust\u001B[0m \u001B[0mmatch\u001B[0m \u001B[0mexpected\u001B[0m \u001B[0mshape\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    168\u001B[0m         \"\"\"\n\u001B[0;32m--> 169\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    170\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    171\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mpipe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstream\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m128\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/check.py\u001B[0m in \u001B[0;36mchecked_function\u001B[0;34m(wrapped, instance, args, kwargs)\u001B[0m\n\u001B[1;32m    149\u001B[0m                     \u001B[0;32mraise\u001B[0m \u001B[0mExpectedTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcheck\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m\"Callable\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    150\u001B[0m                 \u001B[0mcheck\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfix_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 151\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    152\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    153\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0marg_check_adder\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/relu.py\u001B[0m in \u001B[0;36mpredict\u001B[0;34m(self, input__BI)\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mcheck\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhas_shape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"nB\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"nI\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput__BI\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m         \u001B[0moutput__BO\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mAffine\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput__BI\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m         \u001B[0moutput__BO\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput__BO\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minplace\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0moutput__BO\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/check.py\u001B[0m in \u001B[0;36mchecked_function\u001B[0;34m(wrapped, instance, args, kwargs)\u001B[0m\n\u001B[1;32m    149\u001B[0m                     \u001B[0;32mraise\u001B[0m \u001B[0mExpectedTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcheck\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m\"Callable\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    150\u001B[0m                 \u001B[0mcheck\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfix_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 151\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    152\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    153\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0marg_check_adder\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/thinc/neural/_classes/affine.py\u001B[0m in \u001B[0;36mpredict\u001B[0;34m(self, input__BI)\u001B[0m\n\u001B[1;32m     54\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mcheck\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhas_shape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"nB\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"nI\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     55\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput__BI\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 56\u001B[0;31m         \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgemm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput__BI\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mW\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrans2\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     57\u001B[0m         \u001B[0moutput\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     58\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "statement_list = main02_parse_articles.parallel_parse(pl, nlp_eng, stream_art_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}