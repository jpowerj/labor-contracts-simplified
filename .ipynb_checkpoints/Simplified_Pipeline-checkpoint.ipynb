{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 1: Split contract text into individual articles\n",
    "\n",
    "Using the regex method or elliott's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pipeline\n",
    "import plutil\n",
    "\n",
    "from smart_open import s3\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Old: Load from disk\n",
    "#canadian_pt_path = \"/home/research/corpora/contracts/canadian/txt\"\n",
    "#len(os.listdir(canadian_pt_path)) # 44,589 contracts *total*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bucket_name = 'cuecon-textlab'\n",
    "bucket_prefix = 'home/research/corpora/contracts/canadian/txt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#s3_contents = plutil.get_s3_contents(tl_bucket, bucket_prefix)\n",
    "#len(s3_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#for key, content in s3.iter_bucket(bucket, prefix=prefix, accept_key=lambda key: '/201' in key, workers=1, key_limit=3):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full output path: ./output/20220825\n",
      "Loading filenames from cuecon-textlab/home/research/corpora/contracts/canadian/txt/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44590it [00:20, 2200.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 44589; files after language filter (['eng']): 35931\n",
      "35931 filenames loaded (0000102a_eng.txt ... 1498101a_eng.txt)\n"
     ]
    }
   ],
   "source": [
    "#pl = pipeline.Pipeline(\"canadian\", canadian_pt_path, lang_list=[\"eng\"],\n",
    "#                       sample_N=100, splitter=\"elliott\")\n",
    "s3_options = {\n",
    "    'bucket_name': 'cuecon-textlab',\n",
    "    'bucket_prefix': 'home/research/corpora/contracts/canadian/txt/'\n",
    "}\n",
    "pl = pipeline.Pipeline(\"canadian\", mode=\"s3\", mode_options=s3_options,\n",
    "                       output_path=\"./output/\", lang_list=[\"eng\"],\n",
    "                       splitter=\"elliott\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8658"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pl.excluded_fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1074906c_fra.txt', '1265602c_fra.txt')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.excluded_fnames[0], pl.excluded_fnames[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# done!\n",
    "#pl.split_contracts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#import json\n",
    "#fpath = \"../canadian_output/01_artsplit_elliott_json/0000102a.json\"\n",
    "#with open(fpath, 'r') as f:\n",
    "#    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./output/20220825/01_artsplit_elliott_pkl\n"
     ]
    }
   ],
   "source": [
    "# Get the paths to the folders where the pkl and json files should be saved\n",
    "pkl_path = os.path.join(pl.get_output_path(), f\"01_artsplit_{pl.splitter}_pkl\")\n",
    "print(pkl_path)\n",
    "json_path = os.path.join(pl.get_output_path(), f\"01_artsplit_{pl.splitter}_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import detect_sections_elliott as dse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_contract(contract_id, contract_lang, contract_text):\n",
    "    arts, headers = dse.detect_sections(contract_text)\n",
    "    # Convert to the dict format for compatibility with regex splitter\n",
    "    # It looks like len(headers) is almost always greater than (often\n",
    "    # like double or triple) len(arts). So for now I'm ignoring headers\n",
    "    art_list = []\n",
    "    for i in range(len(arts)):\n",
    "        cur_art = arts[i]\n",
    "        cur_art_data = {'header': None, 'text': cur_art, 'section_num': i,\n",
    "                        'contract_id': contract_id, 'lang': contract_lang}\n",
    "        art_list.append(cur_art_data)\n",
    "    return art_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save_json = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|â–Ž                                                                                                                  | 100/35931 [00:10<1:00:08,  9.93it/s]\n"
     ]
    }
   ],
   "source": [
    "accept_rule = lambda fname: fname.endswith('_eng.txt')\n",
    "# Include key_limit=16 for debugging\n",
    "bucket_iter = s3.iter_bucket(bucket_name, prefix=bucket_prefix,\n",
    "                             accept_key=accept_rule, workers=16, key_limit=100)\n",
    "for fpath, content in tqdm(bucket_iter, total=pl.get_num_docs()):\n",
    "    fname = os.path.basename(fpath)\n",
    "    # First we get the info from the filename\n",
    "    fname_data = plutil.parse_fname(fname)\n",
    "    contract_prefix = fname_data['prefix']\n",
    "    contract_id = fname_data['id']\n",
    "    contract_lang = fname_data['lang']\n",
    "    # Check if already processed\n",
    "    pkl_fpath = os.path.join(pkl_path, f\"{contract_prefix}.pkl\")\n",
    "    if os.path.isfile(pkl_fpath):\n",
    "        continue\n",
    "    # Now we process the content\n",
    "    #print(fname, len(content))\n",
    "    contract_text = content.decode('utf-8')\n",
    "    art_list = split_contract(contract_id, contract_lang, contract_text)\n",
    "    # And save the article list as .pkl (for internal use) and .json\n",
    "    # (for human reading)\n",
    "    \n",
    "    plutil.safe_to_pickle(art_list, pkl_fpath)\n",
    "    #print(f\"Saved to {pkl_fpath}\")\n",
    "    if save_json:\n",
    "        json_fpath = os.path.join(json_path, f\"{contract_prefix}.json\")\n",
    "        plutil.safe_to_json(art_list, json_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 2: Parse the articles using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import functools\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# 3rd party imports\n",
    "import joblib\n",
    "\n",
    "# Local imports\n",
    "import pipeline\n",
    "import main02_spacy_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# And set it to work with spacy's use of multiprocessing\n",
    "import multiprocessing_logging\n",
    "multiprocessing_logging.install_mp_handler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load the actual spaCy NLP object (`nlp_eng`), and extend it to include neuralcoref annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Loading spaCy core model\")\n",
    "nlp_eng = spacy.load('en_core_web_md', disable=[\"ner\"])\n",
    "print(\"Loading spaCy coref model. May take a while...\")\n",
    "neuralcoref.add_to_pipe(nlp_eng);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Annoying but necessary additional step: adding \"contract_id\" and \"art_num\" attributes to spacy's Doc class, so that we can serialize and deserialize without headaches [x__x]\n",
    "\n",
    "See https://spacy.io/usage/processing-pipelines#custom-components-attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The force=True is just so that we can change (e.g.) the names or default values and overwrite the extensions\n",
    "# (otherwise this would always cause an Exception)\n",
    "spacy.tokens.Doc.set_extension(\"contract_id\", default=None, force=True)\n",
    "spacy.tokens.Doc.set_extension(\"article_num\", default=None, force=True)\n",
    "spacy.tokens.Doc.set_extension(\"coref_list\", default=[], force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Aaand yet another necessary workaround...\n",
    "\n",
    "See https://github.com/huggingface/neuralcoref/issues/82#issuecomment-569431503\n",
    "\n",
    "[update: holding off on this one actually, since I need the coref data... ugh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cr_test_doc = nlp_eng(u'My sister has a dog. She loves him.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mentions = [\n",
    "    {\n",
    "        \"start\": mention.start_char,\n",
    "        \"end\": mention.end_char,\n",
    "        \"text\": mention.text,\n",
    "        \"resolved\": cluster.main.text,\n",
    "    }\n",
    "    for cluster in cr_test_doc._.coref_clusters\n",
    "    for mention in cluster.mentions\n",
    "]\n",
    "#clusters = list(\n",
    "#    list(span.text for span in cluster)\n",
    "#    for cluster in cr_test_doc._.coref_clusters\n",
    "#)\n",
    "#resolved = cr_test_doc._.coref_resolved\n",
    "#response = {}\n",
    "#response[\"mentions\"] = mentions\n",
    "#response[\"clusters\"] = clusters\n",
    "#response[\"resolved\"] = resolved\n",
    "mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "K I guess we'll use this representation to avoid the serialization errors :|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# All the neuralcoref attributes for a doc, for future reference:\n",
    "#cr_test_doc._.has_coref\n",
    "#cr_test_doc._.coref_resolved\n",
    "#cr_test_doc._.coref_clusters\n",
    "#cr_test_doc._.coref_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#for cluster in cr_test_doc._.coref_clusters:\n",
    "#    print(f\"===== #{cluster.i}\")\n",
    "#    print(cluster)\n",
    "#    print(f\"main: '{cluster.main}'\")\n",
    "#    print(cluster.mentions)\n",
    "#    for mention in cluster.mentions:\n",
    "#        print(mention)\n",
    "#        print(mention.start)\n",
    "#        print(mention.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def stream_art_data(test_N=None):\n",
    "    \"\"\"\n",
    "    test_N: If set to an int, the function will only yield article data for the first `test_N` contracts.\n",
    "            Otherwise, if set to None, article data for all contracts is yielded.\n",
    "    \"\"\"\n",
    "    art_data_fpaths = glob.glob(\"../canadian_output/01_artsplit_elliott_json/*.json\")\n",
    "    # Loop over contracts\n",
    "    for fnum, fpath in enumerate(art_data_fpaths):\n",
    "        if test_N is not None and fnum >= test_N:\n",
    "            # We've already yielded the first `test_N` contracts, so terminate\n",
    "            break\n",
    "        with open(fpath, 'r') as f:\n",
    "            all_articles = json.load(f)\n",
    "        # Now loop over the articles\n",
    "        for cur_article in all_articles:\n",
    "            # We want to yield tuples of (string, {contract_id, article_num})\n",
    "            art_str = cur_article['text']\n",
    "            art_data = {'contract_id':cur_article['contract_id'],\n",
    "                        'article_num':cur_article['section_num']}\n",
    "            yield (art_str, art_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#art_data_fpaths = glob.glob(\"../canadian_output/01_artsplit_elliott_json/*.json\")\n",
    "#first_fpath = art_data_fpaths[0]\n",
    "#with open(first_fpath, 'r') as f:\n",
    "#    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_unserializable_results(doc):\n",
    "    doc.user_data = {}\n",
    "    for x in dir(doc._):\n",
    "        if x in ['get', 'set', 'has']: continue\n",
    "        setattr(doc._, x, None)\n",
    "    for token in doc:\n",
    "        for x in dir(token._):\n",
    "            if x in ['get', 'set', 'has']: continue\n",
    "            setattr(token._, x, None)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_coref_data(doc_obj):\n",
    "    mentions = [\n",
    "        {\n",
    "            \"start\": mention.start_char,\n",
    "            \"end\": mention.end_char,\n",
    "            \"text\": mention.text,\n",
    "            \"resolved\": cluster.main.text,\n",
    "        }\n",
    "        for cluster in doc_obj._.coref_clusters\n",
    "        for mention in cluster.mentions\n",
    "    ]\n",
    "    return mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def transform_texts(nlp, batch_id, batch_tuples, output_dir):\n",
    "    # Using spacy's \"DocBin\" functionality: see https://spacy.io/usage/saving-loading#docs\n",
    "    batch_bin = spacy.tokens.DocBin(store_user_data=True)\n",
    "    #print(nlp.pipe_names)\n",
    "    output_fpath = os.path.join(output_dir, f\"{batch_id}.bin\")\n",
    "    if os.path.isfile(output_fpath):  # return None in case same batch is called again\n",
    "        return None\n",
    "    print(\"Processing batch\", batch_id)\n",
    "    for art_doc, art_meta in nlp.pipe(batch_tuples, as_tuples=True):\n",
    "        # This is the weird part where we now have to change contract_id and art_num\n",
    "        # from being metadata to being attributes of the spacy Doc objects themselves\n",
    "        contract_id = art_meta[\"contract_id\"]\n",
    "        article_num = art_meta[\"article_num\"]\n",
    "        art_doc._.contract_id = contract_id\n",
    "        art_doc._.article_num = article_num\n",
    "        # And now we don't need the meta object anymore, since it's encoded in the Doc itself\n",
    "        # But next we need to get a serializable representation of the detected corefs\n",
    "        art_doc._.coref_list = get_coref_data(art_doc)\n",
    "        # Ok now we can get rid of the original coref attributes that break the data\n",
    "        art_doc = remove_unserializable_results(art_doc)\n",
    "        batch_bin.add(art_doc)\n",
    "    # Now we can use spacy's serialization methods [joblib basically fails at serializing\n",
    "    # spacy Docs for various reasons]\n",
    "    # [see https://spacy.io/usage/saving-loading#docs]\n",
    "    batch_bytes = batch_bin.to_bytes()\n",
    "    # And save the bytes object to file\n",
    "    with open(output_fpath, \"wb\") as f:\n",
    "        f.write(batch_bytes)\n",
    "    print(\"Saved {} texts to {}.bin\".format(len(batch_tuples), batch_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Trying to use multiprocessing like in\n",
    "# https://spacy.io/usage/examples#multi-processing\n",
    "#output_dir = \"./mp_test\"\n",
    "output_dir = \"./mp_full\"\n",
    "#art_tuple_stream = stream_art_data(test_N=50)\n",
    "art_tuple_stream = stream_art_data()\n",
    "\n",
    "print(\"Processing texts...\")\n",
    "batch_size = 1000\n",
    "#batch_size = 200\n",
    "n_jobs = 16\n",
    "art_partitions = spacy.util.minibatch(art_tuple_stream, size=batch_size)\n",
    "executor = joblib.Parallel(n_jobs=n_jobs, backend=\"multiprocessing\", prefer=\"processes\")\n",
    "do = joblib.delayed(functools.partial(transform_texts, nlp_eng))\n",
    "tasks = (do(i, batch_tuples, output_dir) for i, batch_tuples in enumerate(art_partitions))\n",
    "executor(tasks);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Test that it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bin_fpath = \"./mp_test/6.bin\"\n",
    "with open(bin_fpath, \"rb\") as f:\n",
    "    loaded_bytes = f.read()\n",
    "loaded_bin = spacy.tokens.DocBin().from_bytes(loaded_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "doc_iter = loaded_bin.get_docs(nlp_eng.vocab)\n",
    "doc_list = list(doc_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "doc_list[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "processed_arts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#for art_nlp, art_meta in nlp_eng.pipe(stream_art_data(test_N=1), as_tuples=True):\n",
    "#    logger.info(f\"Finished processing: {art_meta}\")\n",
    "#    processed_arts.append((art_nlp, art_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(processed_arts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "type(processed_arts[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "statement_list = main02_parse_articles.parallel_parse(pl, nlp_eng, stream_art_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
